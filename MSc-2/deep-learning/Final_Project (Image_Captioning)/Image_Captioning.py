# -*- coding: utf-8 -*-
"""Image_Captioning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y4T-GBrvlwjIQDVS44E7-OhE6royOseA
"""

# =============================================================================
# IMAGE CAPTIONING PROJECT - FLICKR8K DATASET
# =============================================================================

# =============================================================================
# SECTION 1: DOWNLOAD AND EXTRACT DATASET
# =============================================================================

import os
import urllib.request
import zipfile
from tqdm import tqdm

class DownloadProgressBar(tqdm):
    """Custom progress bar for download"""
    def update_to(self, b=1, bsize=1, tsize=None):
        if tsize is not None:
            self.total = tsize
        self.update(b * bsize - self.n)

def download_url(url, output_path):
    """Download file from URL with progress bar"""
    with DownloadProgressBar(unit='B', unit_scale=True, miniters=1, desc=url.split('/')[-1]) as t:
        urllib.request.urlretrieve(url, filename=output_path, reporthook=t.update_to)

# Create directory for dataset
os.makedirs('/content/Flickr8k', exist_ok=True)

# Download images dataset
print('Downloading Flickr8k Images...')
if not os.path.exists('/content/Flickr8k/Flicker8k_Dataset.zip'):
    download_url(
        'https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip',
        '/content/Flickr8k/Flicker8k_Dataset.zip'
    )
    print('Images downloaded.')
else:
    print('Images already downloaded.')

# Download text dataset
print('Downloading Flickr8k Text...')
if not os.path.exists('/content/Flickr8k/Flickr8k_text.zip'):
    download_url(
        'https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip',
        '/content/Flickr8k/Flickr8k_text.zip'
    )
    print('Text downloaded.')
else:
    print('Text already downloaded.')

# Extract images
print('Extracting Images...')
if not os.path.exists('/content/Flickr8k/Flicker8k_Dataset'):
    with zipfile.ZipFile('/content/Flickr8k/Flicker8k_Dataset.zip', 'r') as zip_ref:
        for file in tqdm(zip_ref.namelist(), desc='Extracting images'):
            zip_ref.extract(file, '/content/Flickr8k')
    print('Images extracted.')
else:
    print('Images already extracted.')

# Extract text files
print('Extracting Text...')
with zipfile.ZipFile('/content/Flickr8k/Flickr8k_text.zip', 'r') as zip_ref:
    for file in tqdm(zip_ref.namelist(), desc='Extracting text'):
        zip_ref.extract(file, '/content/Flickr8k')
print('Text extracted.')

print('Dataset ready!')

# Find required text files
print('\nFinding text files...')
token_file = None
train_file = None

for root, dirs, files in os.walk('/content/Flickr8k'):
    for file in files:
        if file == 'Flickr8k.token.txt':
            token_file = os.path.join(root, file)
            print(f'Found token file: {token_file}')
        if file == 'Flickr_8k.trainImages.txt':
            train_file = os.path.join(root, file)
            print(f'Found train file: {train_file}')

# Validate required files exist
if token_file is None:
    raise FileNotFoundError('Flickr8k.token.txt not found!')
if train_file is None:
    raise FileNotFoundError('Flickr_8k.trainImages.txt not found!')

# =============================================================================
# SECTION 2: IMPORT REQUIRED LIBRARIES
# =============================================================================

import string
import re
import numpy as np
from PIL import Image
from pickle import dump, load
from tensorflow.keras.applications.xception import Xception, preprocess_input
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, Add
import matplotlib.pyplot as plt

# =============================================================================
# SECTION 3: TEXT DATA PREPARATION
# =============================================================================

def load_doc(filename):
    """Load text file into memory"""
    file = open(filename, 'r')
    text = file.read()
    file.close()
    return text

def all_img_captions(filename):
    """Load all image captions from token file"""
    print('Loading image captions...')
    file = load_doc(filename)
    captions = file.split('\n')
    descriptions = {}
    for caption in tqdm(captions[:-1], desc='Processing captions'):
        img, caption = caption.split('\t')
        if img[:-2] not in descriptions:
            descriptions[img[:-2]] = [caption]
        else:
            descriptions[img[:-2]].append(caption)
    return descriptions

def cleaning_text(captions):
    """Clean and preprocess caption text"""
    print('Cleaning text...')
    table = str.maketrans('', '', string.punctuation)
    for img, caps in tqdm(captions.items(), desc='Cleaning captions'):
        for i, img_caption in enumerate(caps):
            # Replace hyphens with spaces
            img_caption = img_caption.replace('-', ' ')
            # Tokenize
            desc = img_caption.split()
            # Convert to lowercase
            desc = [word.lower() for word in desc]
            # Remove punctuation
            desc = [word.translate(table) for word in desc]
            # Remove short words (length < 2)
            desc = [word for word in desc if (len(word) > 1)]
            # Remove tokens with numbers
            desc = [word for word in desc if (word.isalpha())]
            # Join back to string
            img_caption = ' '.join(desc)
            captions[img][i] = img_caption
    return captions

def text_vocabulary(descriptions):
    """Build vocabulary of all unique words"""
    print('Building vocabulary...')
    vocab = set()
    for key in tqdm(descriptions.keys(), desc='Building vocab'):
        [vocab.update(d.split()) for d in descriptions[key]]
    return vocab

def save_descriptions(descriptions, filename):
    """Save descriptions to file"""
    print('Saving descriptions...')
    lines = list()
    for key, desc_list in descriptions.items():
        for desc in desc_list:
            lines.append(key + '\t' + desc)
    data = '\n'.join(lines)
    file = open(filename, 'w')
    file.write(data)
    file.close()

# Load and process captions
filename = '/content/Flickr8k/Flickr8k.token.txt'
descriptions = all_img_captions(filename)
print('Length of descriptions =', len(descriptions))

# Clean descriptions
clean_descriptions = cleaning_text(descriptions)

# Build vocabulary
vocabulary = text_vocabulary(clean_descriptions)
print('Length of vocabulary =', len(vocabulary))

# Save cleaned descriptions
save_descriptions(clean_descriptions, 'descriptions.txt')

# =============================================================================
# SECTION 4: IMAGE FEATURE EXTRACTION
# =============================================================================

def extract_features(directory):
    """Extract features from images using Xception model"""
    print('Loading Xception model...')
    model = Xception(include_top=False, pooling='avg')
    features = {}
    image_list = os.listdir(directory)
    print(f'Extracting features from {len(image_list)} images...')
    for img in tqdm(image_list, desc='Extracting features'):
        filename = directory + '/' + img
        # Load and preprocess image
        image = Image.open(filename)
        image = image.resize((299, 299))
        image = np.expand_dims(image, axis=0)
        # Normalize pixel values
        image = image / 127.5
        image = image - 1.0
        # Extract features
        feature = model.predict(image, verbose=0)
        features[img] = feature
    return features

# Extract features from all images
dataset_images = '/content/Flickr8k/Flicker8k_Dataset'
features = extract_features(dataset_images)
print('Saving features...')
dump(features, open('features.p', 'wb'))
print('Features saved!')

# =============================================================================
# SECTION 5: LOAD TRAINING DATA
# =============================================================================

def load_photos(filename):
    """Load list of photo identifiers"""
    print('Loading photo list...')
    file = load_doc(filename)
    photos = file.split('\n')[:-1]
    return photos

def load_clean_descriptions(filename, photos):
    """Load descriptions for specific photos with start/end tokens"""
    print('Loading clean descriptions...')
    file = load_doc(filename)
    descriptions = {}
    for line in tqdm(file.split('\n'), desc='Loading descriptions'):
        words = line.split()
        if len(words) < 1:
            continue
        image, image_caption = words[0], words[1:]
        if image in photos:
            if image not in descriptions:
                descriptions[image] = []
            # Add start and end tokens
            desc = '<start> ' + ' '.join(image_caption) + ' <end>'
            descriptions[image].append(desc)
    return descriptions

def load_features(photos):
    """Load features for specific photos"""
    print('Loading features...')
    all_features = load(open('features.p', 'rb'))
    features = {k: all_features[k] for k in photos}
    return features

# Load training data
filename = '/content/Flickr8k/Flickr_8k.trainImages.txt'
train_imgs = load_photos(filename)
train_descriptions = load_clean_descriptions('descriptions.txt', train_imgs)
train_features = load_features(train_imgs)

# =============================================================================
# SECTION 6: TOKENIZATION AND VOCABULARY
# =============================================================================

def dict_to_list(descriptions):
    """Convert descriptions dictionary to list"""
    all_desc = []
    for key in descriptions.keys():
        [all_desc.append(d) for d in descriptions[key]]
    return all_desc

def create_tokenizer(descriptions):
    """Create tokenizer from descriptions"""
    print('Creating tokenizer...')
    desc_list = dict_to_list(descriptions)
    tokenizer = Tokenizer()
    tokenizer.fit_on_texts(desc_list)
    return tokenizer

def calculate_max_length(descriptions):
    """Calculate maximum description length"""
    desc_list = dict_to_list(descriptions)
    return max(len(d.split()) for d in desc_list)

# Create and save tokenizer
tokenizer = create_tokenizer(train_descriptions)
dump(tokenizer, open('tokenizer.p', 'wb'))
print('Tokenizer saved!')

max_length_value = calculate_max_length(train_descriptions)
print(f'Max length: {max_length_value}')

dump(max_length_value, open('max_length.p', 'wb'))
print('✓ max_length.p saved!')

vocab_size = len(tokenizer.word_index) + 1
print(f'Vocabulary size: {vocab_size}')

# =============================================================================
# SECTION 7: DATA GENERATOR
# =============================================================================

def create_sequences(tokenizer, max_length, desc_list, feature, vocab_size):
    """Create input-output sequence pairs for training"""
    X1, X2, y = list(), list(), list()
    for desc in desc_list:
        # Encode sequence
        seq = tokenizer.texts_to_sequences([desc])[0]
        # Split into input and output pairs
        for i in range(1, len(seq)):
            in_seq, out_seq = seq[:i], seq[i]
            # Pad input sequence
            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]
            # Encode output sequence
            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]
            # Store
            X1.append(feature)
            X2.append(in_seq)
            y.append(out_seq)
    return np.array(X1), np.array(X2), np.array(y)

# =============================================================================
# SECTION 8: MODEL DEFINITION
# =============================================================================

def define_model(vocab_size, max_length):
    """Define the image captioning model"""
    print('Building model...')
    # Image feature extractor
    inputs1 = Input(shape=(2048,))
    fe1 = Dropout(0.5)(inputs1)
    fe2 = Dense(256, activation='relu')(fe1)

    # Sequence feature extractor
    inputs2 = Input(shape=(max_length,))
    se1 = Embedding(vocab_size, 256, mask_zero=False)(inputs2)
    se2 = Dropout(0.5)(se1)
    se3 = LSTM(256)(se2)

    # Decoder (merge both models)
    Merge = Add()([fe2, se3])
    decoder = Dense(256, activation='relu')(Merge)
    outputs = Dense(vocab_size, activation='softmax')(decoder)

    # Compile model
    model = Model(inputs=[inputs1, inputs2], outputs=outputs)
    model.compile(loss='categorical_crossentropy', optimizer='adam')
    print(model.summary())
    return model

# =============================================================================
# SECTION 9: TRAINING CONFIGURATION
# =============================================================================

print('\n' + '='*50)
print('TRAINING CONFIGURATION')
print('='*50)
print('Dataset:', len(train_imgs))
print('Descriptions: train =', len(train_descriptions))
print('Photos: train =', len(train_features))
print('Description length:', max_length_value)
print('Vocabulary size:', vocab_size)
print('='*50 + '\n')

# Build model
model = define_model(vocab_size, max_length_value)

# Training parameters
epochs = 10
os.makedirs('models', exist_ok=True)

# =============================================================================
# SECTION 10: MODEL TRAINING
# =============================================================================

print('\n' + '='*50)
print('STARTING TRAINING')
print('='*50 + '\n')

for i in range(epochs):
    print(f'\n{"="*50}')
    print(f'EPOCH {i+1}/{epochs}')
    print(f'{"="*50}')

    # Train on each image
    for key in tqdm(train_descriptions.keys(), desc=f'Epoch {i+1}/{epochs}'):
        description_list = train_descriptions[key]
        feature = train_features[key][0]
        # Create sequences
        input_image, input_sequence, output_word = create_sequences(
            tokenizer, max_length_value, description_list, feature, vocab_size
        )
        # Train on batch
        model.train_on_batch([input_image, input_sequence], output_word)

    # Save model after each epoch
    print(f'\nSaving model {i}...')
    model.save(f'models/model_{i}.h5')
    print(f'✓ Model {i} saved successfully!')

print('\n' + '='*50)
print('TRAINING COMPLETED!')
print('='*50 + '\n')

# =============================================================================
# SECTION 11: PREDICTION FUNCTIONS
# =============================================================================

def extract_features_single(filename, model):
    """Extract features from a single image"""
    image = Image.open(filename)
    image = image.resize((299, 299))
    image = np.expand_dims(image, axis=0)
    # Normalize
    image = image / 127.5
    image = image - 1.0
    # Extract features
    feature = model.predict(image, verbose=0)
    return feature

def word_for_id(integer, tokenizer):
    """Map integer to word using tokenizer"""
    for word, index in tokenizer.word_index.items():
        if index == integer:
            return word
    return None

def generate_desc(model, tokenizer, photo, max_length):
    """Generate description for an image"""
    in_text = '<start>'

    for i in range(max_length):
        sequence = tokenizer.texts_to_sequences([in_text])[0]
        sequence = pad_sequences([sequence], maxlen=max_length)
        pred = model.predict([photo, sequence], verbose=0)
        pred = np.argmax(pred)
        word = word_for_id(pred, tokenizer)

        if word is None:
            break

        in_text += ' ' + word

        if word == '<end>' or word == 'end':
            break

    final_caption = re.sub(r'<start>|<end>|\bstart\b|\bend\b', '', in_text)
    final_caption = re.sub(r'\s+', ' ', final_caption).strip()

    return final_caption

# =============================================================================
# SECTION 12: MODEL TESTING
# =============================================================================

print('\n' + '='*50)
print('TESTING MODEL')
print('='*50 + '\n')

# Load test image path
path = '/content/Flickr8k/Flicker8k_Dataset/3433567526_00b5a70319.jpg'

# Load required files
print('Loading model and tokenizer...')
max_length = load(open('max_length.p', 'rb'))
tokenizer = load(open('tokenizer.p', 'rb'))
model = load_model('models/model_9.h5')
Xception_model = Xception(include_top=False, pooling='avg')
print('✓ Model loaded!')

# Extract features and generate caption
print('Extracting features...')
photo = extract_features_single(path, Xception_model)
print('✓ Features extracted!')

print('Generating caption...')
img = Image.open(path)
description = generate_desc(model, tokenizer, photo, max_length)
print('✓ Caption generated!')

# Display results
print('\n' + '='*50)
print('RESULT')
print('='*50)
print(f'Caption: {description}')
print('='*50 + '\n')

# Show image with caption
plt.figure(figsize=(10, 8))
plt.imshow(img)
plt.axis('off')
plt.title(description, fontsize=14, wrap=True)
plt.show()