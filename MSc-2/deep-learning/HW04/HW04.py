# -*- coding: utf-8 -*-
"""HW04.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19aPZJGPr4CqvQAZjbetPP1DehjozvW4T
"""

print("Installing comet_ml...")
!pip install comet_ml --quiet
print("Installation complete!\n")

import comet_ml
import tensorflow as tf
from tensorflow import keras
import numpy as np
import pathlib
import matplotlib.pyplot as plt

from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
from tensorflow.keras.losses import SparseCategoricalCrossentropy

print(f"TensorFlow version: {tf.__version__}")
print(f"GPU device: {tf.test.gpu_device_name()}")
print()

comet_ml.login()

optimizers_dict = {
    "adam": keras.optimizers.Adam(learning_rate=0.001),
    "rmsprop": keras.optimizers.RMSprop(learning_rate=0.001),
    "sgd": keras.optimizers.SGD(learning_rate=0.001, momentum=0.9),
    "nadam": keras.optimizers.Nadam(learning_rate=0.001),
    "adagrad": keras.optimizers.Adagrad(learning_rate=0.001)
}

print("Downloading flower photos dataset...")
dataset_url = "https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz"

archive_path = tf.keras.utils.get_file(
    origin=dataset_url,
    fname="flower_photos.tgz",
    extract=True
)

base_dir = pathlib.Path(archive_path).parent

data_dir = None
possible_paths = [
    base_dir / "flower_photos",
    pathlib.Path(str(base_dir).replace("/flower_photos", "")) / "flower_photos"
]

for path in possible_paths:
    if path.exists() and (path / "daisy").exists():
        data_dir = path
        break

if data_dir is None:
    all_subdirs = list(base_dir.rglob("daisy"))
    if all_subdirs:
        data_dir = all_subdirs[0].parent

if data_dir is None or not data_dir.exists():
    raise FileNotFoundError(f"Dataset directory not found. Searched in: {base_dir}")

print(f"Dataset path: {data_dir}")

subdirs = sorted([d.name for d in data_dir.iterdir() if d.is_dir()])
print(f"Subdirectories: {subdirs}")

total_images = 0
for subdir in subdirs:
    num_images = len(list((data_dir / subdir).glob("*.jpg")))
    total_images += num_images
    print(f"   - {subdir}: {num_images} images")
print(f"   Total: {total_images} images")
print()

BATCH_SIZE = 32
img_height = 180
img_width = 180

def get_dataset():
    train_ds = tf.keras.utils.image_dataset_from_directory(
        data_dir,
        validation_split=0.2,
        subset="training",
        seed=123,
        image_size=(img_height, img_width),
        batch_size=BATCH_SIZE
    )

    val_ds = tf.keras.utils.image_dataset_from_directory(
        data_dir,
        validation_split=0.2,
        subset="validation",
        seed=123,
        image_size=(img_height, img_width),
        batch_size=BATCH_SIZE
    )

    class_names = train_ds.class_names
    num_classes = len(class_names)

    print(f"Classes: {class_names}")
    print(f"Number of classes: {num_classes}")

    if num_classes != 5:
        raise ValueError(f"Expected 5 classes but found {num_classes}!")

    return train_ds, val_ds, class_names, num_classes

train_ds, val_ds, class_names, num_classes = get_dataset()
print()

data_augmentation = Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.1),
    layers.RandomZoom(0.1),
    layers.RandomTranslation(height_factor=0.2, width_factor=0.2)
], name="data_augmentation")

def prepare_train(ds):
    ds = ds.map(lambda x, y: (data_augmentation(x, training=True), y),
                num_parallel_calls=tf.data.AUTOTUNE)
    ds = ds.map(lambda x, y: (x / 255.0, y),
                num_parallel_calls=tf.data.AUTOTUNE)
    return ds

def prepare_val(ds):
    ds = ds.map(lambda x, y: (x / 255.0, y),
                num_parallel_calls=tf.data.AUTOTUNE)
    return ds

train_ds = prepare_train(train_ds)
val_ds = prepare_val(val_ds)

AUTOTUNE = tf.data.AUTOTUNE
train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)

print("Dataset preprocessing complete!")
print("   Training: with data augmentation + normalization")
print("   Validation: with normalization only (no augmentation)")
print()

print("Visualizing data augmentation examples...")

vis_ds = tf.keras.utils.image_dataset_from_directory(
    data_dir,
    validation_split=0.2,
    subset="training",
    seed=123,
    image_size=(img_height, img_width),
    batch_size=BATCH_SIZE
)

plt.figure(figsize=(12, 12))

for images, labels in vis_ds.take(1):
    single_image = images[0:1] / 255.0
    class_name = class_names[labels[0].numpy()]

    for i in range(9):
        augmented = data_augmentation(single_image, training=True)

        ax = plt.subplot(3, 3, i + 1)
        plt.imshow(augmented[0])
        plt.axis('off')
        plt.title(f"Aug {i+1}", fontsize=10)

    plt.suptitle(f"Data Augmentation - {class_name.upper()}",
                 fontsize=16, fontweight='bold')
    break

plt.tight_layout()
plt.show()
print()

def build_model_graph(experiment):
    optz_name = experiment.get_parameter("optz")
    optimizer = optimizers_dict[optz_name]

    model = Sequential([
        layers.Input(shape=(img_height, img_width, 3)),

        layers.Conv2D(32, (3, 3), activation='relu'),
        layers.BatchNormalization(),
        layers.MaxPooling2D((2, 2)),

        layers.Conv2D(64, (3, 3), activation='relu'),
        layers.BatchNormalization(),
        layers.MaxPooling2D((2, 2)),

        layers.Conv2D(128, (3, 3), activation='relu'),
        layers.BatchNormalization(),
        layers.MaxPooling2D((2, 2)),

        layers.Conv2D(256, (3, 3), activation='relu'),
        layers.BatchNormalization(),
        layers.MaxPooling2D((2, 2)),

        layers.Flatten(),
        layers.Dense(512, activation='relu'),
        layers.Dropout(0.5),
        layers.Dense(256, activation='relu'),
        layers.Dropout(0.3),
        layers.Dense(num_classes)
    ], name="flower_classifier")

    model.compile(
        optimizer=optimizer,
        loss=SparseCategoricalCrossentropy(from_logits=True),
        metrics=['accuracy']
    )

    return model

def train(experiment, model, train_ds, val_ds):
    epochs = experiment.get_parameter("epochs")

    early_stopping = keras.callbacks.EarlyStopping(
        monitor='val_accuracy',
        patience=7,
        restore_best_weights=True,
        mode='max'
    )

    reduce_lr = keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=3,
        min_lr=1e-7,
        verbose=1
    )

    history = model.fit(
        train_ds,
        validation_data=val_ds,
        epochs=epochs,
        callbacks=[early_stopping, reduce_lr],
        verbose=1
    )

    return history

config = {
    "algorithm": "bayes",
    "name": "Optimize Flower Classifier",
    "spec": {
        "maxCombo": 5,
        "objective": "maximize",
        "metric": "val_accuracy"
    },
    "parameters": {
        "optz": {
            "type": "categorical",
            "values": ["adam", "rmsprop", "sgd", "nadam", "adagrad"]
        },
    },
    "trials": 1,
}

print("="*70)
print("Starting Comet ML Optimization")
print("="*70)
print()

opt = comet_ml.Optimizer(config, project_name="comet-optimizer")

for experiment in opt.get_experiments():
    experiment.log_parameter("epochs", 50)
    experiment.log_parameter("batch_size", BATCH_SIZE)
    experiment.log_parameter("img_height", img_height)
    experiment.log_parameter("img_width", img_width)
    experiment.log_parameter("num_classes", num_classes)
    experiment.log_other("class_names", class_names)

    optz_name = experiment.get_parameter("optz")

    print(f"\n{'='*70}")
    print(f"Experiment with optimizer: {optz_name.upper()}")
    print(f"{'='*70}\n")

    model = build_model_graph(experiment)
    model.summary()
    print()

    history = train(experiment, model, train_ds, val_ds)

    best_epoch = np.argmax(history.history['val_accuracy'])
    final_loss = history.history['loss'][best_epoch]
    final_acc = history.history['accuracy'][best_epoch]
    final_val_loss = history.history['val_loss'][best_epoch]
    final_val_acc = history.history['val_accuracy'][best_epoch]

    print(f"\n{'='*70}")
    print(f"Best Results (Epoch {best_epoch + 1}):")
    print(f"{'='*70}")
    print(f"   Training Loss:       {final_loss:.4f}")
    print(f"   Training Accuracy:   {final_acc*100:.2f}%")
    print(f"   Validation Loss:     {final_val_loss:.4f}")
    print(f"   Validation Accuracy: {final_val_acc*100:.2f}%")
    print(f"{'='*70}\n")

    experiment.end()

    print(f"Experiment with {optz_name} completed!\n")

print("="*70)
print("All optimization experiments complete!")
print("="*70)
print()

print("Testing model with custom image...")
img_path = "/content/flower.jpg"

try:
    img = tf.keras.utils.load_img(img_path, target_size=(img_height, img_width))
    img_array = tf.keras.utils.img_to_array(img)
    img_array = tf.expand_dims(img_array, 0)
    img_array = img_array / 255.0

    predictions = model.predict(img_array, verbose=0)
    score = tf.nn.softmax(predictions[0])

    predicted_class = class_names[np.argmax(score)]
    confidence = 100 * np.max(score)

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

    ax1.imshow(img)
    ax1.axis('off')

    if confidence > 80:
        title_color = 'green'
    elif confidence > 60:
        title_color = 'orange'
    else:
        title_color = 'red'

    ax1.set_title(
        f"{predicted_class.upper()}\nConfidence: {confidence:.1f}%",
        fontsize=16,
        fontweight='bold',
        color=title_color
    )

    probs = [100 * score[i].numpy() for i in range(num_classes)]
    colors = ['#2ecc71' if i == np.argmax(score) else '#3498db' for i in range(num_classes)]

    bars = ax2.barh(class_names, probs, color=colors, edgecolor='black', linewidth=1.5)
    ax2.set_xlabel('Confidence (%)', fontsize=12, fontweight='bold')
    ax2.set_title('Class Probabilities', fontsize=14, fontweight='bold')
    ax2.set_xlim(0, 100)
    ax2.grid(axis='x', alpha=0.3, linestyle='--')

    for i, (bar, prob) in enumerate(zip(bars, probs)):
        ax2.text(prob + 2, i, f'{prob:.1f}%', va='center', fontsize=11, fontweight='bold')

    plt.tight_layout()
    plt.show()

    print(f"\n{'='*70}")
    print(f"PREDICTION RESULT")
    print(f"{'='*70}")
    print(f"Class:      {predicted_class.upper()}")
    print(f"Confidence: {confidence:.2f}%")
    print(f"Status:     {'High confidence' if confidence > 70 else 'Low confidence'}")
    print(f"{'='*70}\n")

    print("Detailed class probabilities:")
    print("-" * 70)
    sorted_indices = np.argsort(probs)[::-1]
    for rank, idx in enumerate(sorted_indices, 1):
        name = class_names[idx]
        prob = probs[idx]
        bar_length = int(prob / 2)
        bar = '=' * bar_length
        marker = ' <- PREDICTED' if idx == np.argmax(score) else ''
        print(f"{rank}. {name:15s}: {prob:5.1f}% {bar}{marker}")
    print("-" * 70)

except FileNotFoundError:
    print(f"\nError: Image not found at {img_path}\n")
    print("=" * 70)
    print("HOW TO FIX:")
    print("=" * 70)
    print("Option 1: Upload your own flower image to /content/flower.jpg")
    print()
    print("Option 2: Download a sample image")
    print("Run this code:")
    print("!wget -O /content/flower.jpg https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos/roses/2414954629_3708a1a04d.jpg")
    print("=" * 70)

except Exception as e:
    print(f"\nError: {e}")
    import traceback
    traceback.print_exc()

print("\n" + "="*70)
print("ALL TASKS COMPLETED!")
print("="*70)