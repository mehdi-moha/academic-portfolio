# -*- coding: utf-8 -*-
"""CV_Final_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eya7RMEtUXip8FWkG9WJr-FlTkuthNLr
"""

# ==========================================
# Execute this cell once to prepare the dataset in your Drive
# ==========================================

import os
import zipfile
from google.colab import drive

# 1. Mount Google Drive
drive.mount("/content/drive")

# 2. Define directory paths
base_dir = "/content/drive/MyDrive/traffic-signs-data/"

# 3. Create directory if it doesn't exist
if not os.path.exists(base_dir):
    os.makedirs(base_dir)
    print(f"Created directory: {base_dir}")
else:
    print(f"Directory already exists: {base_dir}")

# Change working directory
os.chdir(base_dir)

# 4. Download Dataset (only if not present)
print("Checking for dataset files...")
if not os.path.exists("traffic-signs-data.zip"):
    print("Downloading traffic-signs-data.zip...")
    !wget -N https://d17h27t6h515a5.cloudfront.net/topher/2017/February/5898cd6f_traffic-signs-data/traffic-signs-data.zip

    print("Extracting files...")
    with zipfile.ZipFile("traffic-signs-data.zip", "r") as zip_ref:
        zip_ref.extractall(base_dir)
else:
    print("Zip file already exists.")

# 5. Download Sign Names CSV (Raw version)
if not os.path.exists("signnames.csv"):
    print("Downloading signnames.csv...")
    !wget -N https://raw.githubusercontent.com/AvivSham/German-Traffic-Signs-Classification/master/signnames.csv
else:
    print("CSV file already exists.")

print("\nSetup Complete! Files in directory:")
!ls

# ==========================================
# 1. Imports and Configuration
# ==========================================
import pickle
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import random
import tensorflow as tf
from tensorflow import keras
from keras import layers
from sklearn.utils import shuffle
from sklearn.metrics import confusion_matrix, classification_report
import warnings

# Suppress warnings for cleaner output
warnings.filterwarnings("ignore")

# Mount Google Drive (if not already mounted)
from google.colab import drive
drive.mount("/content/drive")

# Constants
DATA_PATH = "/content/drive/MyDrive/traffic-signs-data/"
MODEL_SAVE_PATH = DATA_PATH + "traffic_sign_model.h5"
BATCH_SIZE = 32
EPOCHS = 12
LEARNING_RATE = 0.001

# ==========================================
# 2. Loading Data
# ==========================================
print("\n--- Loading Data ---")

# Load pickle files
with open(DATA_PATH + "train.p", mode="rb") as f:
    train_data = pickle.load(f)
with open(DATA_PATH + "valid.p", mode="rb") as f:
    valid_data = pickle.load(f)
with open(DATA_PATH + "test.p", mode="rb") as f:
    test_data = pickle.load(f)

# Load Label Names (CSV)
# This allows us to map Class ID (0, 1, 2...) to Name (Speed Limit 20...)
sign_names = pd.read_csv(DATA_PATH + "signnames.csv")
print("Sign names loaded successfully.")

# Extract features and labels
X_train, y_train = train_data["features"], train_data["labels"]
X_valid, y_valid = valid_data["features"], valid_data["labels"]
X_test, y_test = test_data["features"], test_data["labels"]

print(f"Training set shape: {X_train.shape}")
print(f"Validation set shape: {X_valid.shape}")
print(f"Test set shape: {X_test.shape}")

# ==========================================
# 3. Preprocessing
# ==========================================
print("\n--- Preprocessing Data ---")

# 3.1. Shuffle training data
X_train, y_train = shuffle(X_train, y_train)

# 3.2. Grayscale Conversion
# Calculating the average of RGB channels
# Output shape becomes (N, 32, 32, 1)
X_train_gray = np.sum(X_train / 3, axis=3, keepdims=True)
X_valid_gray = np.sum(X_valid / 3, axis=3, keepdims=True)
X_test_gray = np.sum(X_test / 3, axis=3, keepdims=True)

# 3.3. Normalization
# Centering pixels around 0 (Range: -1 to 1)
X_train_norm = (X_train_gray - 128) / 128
X_valid_norm = (X_valid_gray - 128) / 128
X_test_norm = (X_test_gray - 128) / 128

# Get input shape for the CNN
input_shape = X_train_norm[0].shape
print(f"Processed input shape: {input_shape}")

# ==========================================
# 4. Model Architecture (CNN)
# ==========================================
print("\n--- Building Model ---")

model = tf.keras.Sequential([
    # Convolutional Block 1
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D(pool_size=(2, 2)),
    layers.Dropout(0.25),

    # Flattening
    layers.Flatten(),

    # Dense Layers
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.5),

    # Output Layer
    # CRITICAL FIX: Using 'softmax' for multi-class classification (43 classes)
    layers.Dense(43, activation='softmax')
])

# Compile the model
model.compile(loss='sparse_categorical_crossentropy',
              optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),
              metrics=['accuracy'])

model.summary()

# ==========================================
# 5. Training
# ==========================================
print("\n--- Starting Training ---")

history = model.fit(X_train_norm, y_train,
                    batch_size=BATCH_SIZE,
                    epochs=EPOCHS,
                    verbose=1,
                    validation_data=(X_valid_norm, y_valid))

# Save the trained model
model.save(MODEL_SAVE_PATH)
print(f"Model saved to: {MODEL_SAVE_PATH}")

# ==========================================
# 6. Evaluation & Visualization
# ==========================================
print("\n--- Evaluating Model ---")

# 6.1. Plot Training History
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs_range = range(1, len(acc) + 1)

plt.figure(figsize=(12, 5))

# Accuracy Plot
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)

# Loss Plot
plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

# 6.2. Evaluate on Test Data
test_loss, test_accuracy = model.evaluate(X_test_norm, y_test, verbose=0)
print(f"Test Accuracy: {test_accuracy:.4f}")

# 6.3. Classification Report
predictions = np.argmax(model.predict(X_test_norm), axis=1)
print("\nClassification Report:\n")
print(classification_report(y_test, predictions))

# ==========================================
# 7. Visualizing Predictions with Names
# ==========================================
print("\n--- Visualizing Predictions ---")

# Create a dictionary to map ClassId to SignName
label_map = dict(zip(sign_names['ClassId'], sign_names['SignName']))

plt.figure(figsize=(15, 18))

for i in range(12):
    # Select random image from test set
    index = random.randint(0, len(X_test_norm) - 1)

    plt.subplot(4, 3, i + 1)

    # Display image (cmap='gray' is essential for correct visualization)
    plt.imshow(X_test_norm[index].squeeze(), cmap='gray', interpolation='none')

    # Get prediction and truth
    pred_id = predictions[index]
    true_id = y_test[index]

    # Map IDs to Names
    pred_name = label_map[pred_id]
    true_name = label_map[true_id]

    # Set color: Green if correct, Red if wrong
    color = "green" if pred_id == true_id else "red"

    plt.title(f"Pred: {pred_name}\nTrue: {true_name}", color=color, fontsize=10)
    plt.axis('off')

plt.tight_layout()
plt.show()