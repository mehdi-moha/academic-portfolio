# -*- coding: utf-8 -*-
"""Final_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B41d1yQXhSETrxOwQRbsWbUQokxF6Zwo
"""

#!/usr/bin/env python
# coding: utf-8

import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.compose import make_column_transformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.feature_selection import SelectFromModel
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from imblearn.over_sampling import SMOTE

# ============================================================
# 1) Load and clean data
# ============================================================
df0 = pd.read_csv("diabetes_prediction_dataset.csv")
print("Head of raw data:")
print(df0.head(10))

print("\nColumns:", df0.columns.to_list())
print("\nDtypes:")
print(df0.dtypes)
print("\nTarget distribution (raw):")
print(df0["diabetes"].value_counts())

# Drop duplicated rows if any
if np.sum(df0.duplicated() == True) != 0:
    df0 = df0.drop_duplicates()

# Drop rows with any missing values (simple strategy)
if df0.isnull().sum().sum() != 0:
    df0 = df0.dropna()

# Encode gender
print("\nGender value counts (before cleaning):")
print(df0["gender"].value_counts())

# Remove "Other" gender (very few samples)
df0 = df0[df0["gender"] != "Other"]

# Map gender to numeric 0/1 (Female/Male)
df0["gender"].replace({"Female": 0, "Male": 1}, inplace=True)

print("\nSmoking_history value counts (raw):")
print(df0["smoking_history"].value_counts())

def recategorize_smoking(smoking_status):
    """Map original smoking_history values into 3 macro categories."""
    if smoking_status in ["never", "No Info"]:
        return "non-smoker"
    elif smoking_status == "current":
        return "current"
    elif smoking_status in ["ever", "former", "not current"]:
        return "past_smoker"

df0["smoking_history"] = df0["smoking_history"].apply(recategorize_smoking)
print("\nHead after gender/smoking recoding:")
print(df0.head(10))

# ============================================================
# 2) One-Hot encoding for smoking_history
# ============================================================
mct = make_column_transformer(
    (OneHotEncoder(sparse_output=False), ["smoking_history"]),
    remainder="passthrough",
)

df1 = pd.DataFrame(
    mct.fit_transform(df0),
    columns=[
        "current",
        "non-smoker",
        "past_smoker",
        "gender",
        "age",
        "hypertension",
        "heart_disease",
        "bmi",
        "HbA1c_level",
        "blood_glucose_level",
        "diabetes",
    ],
)

print("\nHead of processed dataframe (df1):")
print(df1.head(10))

print("\nSummary of main continuous features:")
print(df1[["age", "bmi", "HbA1c_level", "blood_glucose_level"]].describe())

# ============================================================
# 3) Histograms for all features (visual EDA)
# ============================================================
plt.figure(figsize=(30, 15))
try:
    colors = [
        ["#FF00AE"],
        ["#D0FF00"],
        ["#3AE2CE"],
        ["#00FFA9"],
        ["#FFDD00"],
        ["#B34EE9"],
        ["#0FC0FC"],
        ["#FF0800"],
        ["#00FF00"],
        ["#FFB4C8"],
        ["#00FFFF"],
    ]
    for i, col in enumerate(df1.columns.to_list()):
        plt.subplot(3, 4, i + 1)
        freq, bins, patches = plt.hist(
            df1[col], label=col, color=colors[i], edgecolor="black"
        )
        bin_centers = np.diff(bins) * 0.5 + bins[:-1]
        n = 0
        for fr, x, patch in zip(freq, bin_centers, patches):
            height = int(freq[n])
            plt.annotate(
                "{}".format(height),
                xy=(x, height),
                xytext=(0, 0.2),
                textcoords="offset points",
                ha="center",
                va="bottom",
            )
            n += 1
        plt.legend()
        plt.title(col)
        plt.tight_layout()
except Exception as e:
    print(col, e)
plt.show()

# ============================================================
# 4) Convert to NumPy, split, SMOTE, scaling
# ============================================================
data = df1.to_numpy()
X = data[:, :10]
Y = data[:, 10]

X_train, X_test, Y_train, Y_test = train_test_split(
    X, Y, random_state=42, test_size=0.25, stratify=Y
)
print("\nTrain/Test shapes before SMOTE:")
print("X_train:", X_train.shape, "X_test:", X_test.shape)

# Balance classes with SMOTE
sm = SMOTE(random_state=42)
X_train, Y_train = sm.fit_resample(X_train, Y_train)
print("X_train after SMOTE:", X_train.shape)

# Scale only continuous numeric features: age, bmi, HbA1c, blood_glucose
scaler = StandardScaler()
scaler.fit(X_train[:, [4, 7, 8, 9]])
X_train_scaled = scaler.transform(X_train[:, [4, 7, 8, 9]])
X_test_scaled = scaler.transform(X_test[:, [4, 7, 8, 9]])
X_train[:, [4, 7, 8, 9]] = X_train_scaled
X_test[:, [4, 7, 8, 9]] = X_test_scaled

# ============================================================
# 5) Correlation and categorical analysis
# ============================================================
df_train_full = pd.DataFrame(
    np.concatenate((X_train, Y_train[:, None]), axis=1),
    columns=[
        "current",
        "non-smoker",
        "past_smoker",
        "gender",
        "age",
        "hypertension",
        "heart_disease",
        "bmi",
        "HbA1c_level",
        "blood_glucose_level",
        "diabetes",
    ],
)

# Correlation for numeric features
numeric_cols = ["age", "bmi", "HbA1c_level", "blood_glucose_level", "diabetes"]
corr_numeric = df_train_full[numeric_cols].corr()

plt.figure(figsize=(6, 4))
sns.heatmap(corr_numeric, cmap="hsv", vmin=-1, vmax=1, annot=True)
plt.title("Correlation (numeric features only)")
plt.tight_layout()
plt.show()

print("\nCorrelation of numeric features with diabetes:")
print(corr_numeric["diabetes"].sort_values(ascending=False))

# Categorical vs diabetes: aggregated (numeric) view
gender_means = df_train_full.groupby("gender")["diabetes"].mean()
smoking_means = df0.groupby("smoking_history")["diabetes"].mean()

plt.figure(figsize=(10, 4))

# Gender
plt.subplot(1, 2, 1)
plt.bar(
    ["Female", "Male"],
    [gender_means.loc[0], gender_means.loc[1]],
    color=["#FF00AE", "#00FFA9"],
    edgecolor="black",
)
plt.title("P(diabetes=1) by gender")
plt.ylabel("P(diabetes=1)")

# Smoking history
labels = ["non-smoker", "current", "past_smoker"]
values = [
    smoking_means.loc["non-smoker"],
    smoking_means.loc["current"],
    smoking_means.loc["past_smoker"],
]
plt.subplot(1, 2, 2)
plt.bar(
    labels,
    values,
    color=["#FFDD00", "#3AE2CE", "#B34EE9"],
    edgecolor="black",
)
plt.title("P(diabetes=1) by smoking history")
plt.ylabel("P(diabetes=1)")

plt.tight_layout()
plt.show()

# ============================================================
# 6) Feature importance with ExtraTrees (embedded selection)
# ============================================================
clf = ExtraTreesClassifier(n_estimators=100)
clf = clf.fit(X_train, Y_train)

model = SelectFromModel(clf, prefit=True)
Xtr = model.transform(X_train)
Ytr = Y_train

importance = clf.feature_importances_
important_features = [str(i) for i, v in enumerate(importance) if v > np.mean(importance)]
print("\nImportant features (index > mean importance):", important_features)

# Build Xte using the same feature indices
row = X_test.shape[0]
col = len(important_features)
Xte = np.zeros((row, col))
for idx, j in enumerate(important_features):
    Xte[:, idx] = X_test[:, int(j)]
Yte = Y_test

print("Shape after feature selection: Xtr:", Xtr.shape, "Xte:", Xte.shape)

print("\nExtraTrees feature importances:")
for i, v in enumerate(importance):
    print("Feature: %0d, Score: %.5f" % (i, v))

plt.bar(
    [x for x in range(len(importance))],
    importance,
    color="#FF41A0",
    edgecolor="black",
    width=0.55,
)
plt.axhline(np.mean(importance), color="#0048BA", linestyle="--")
plt.title("ExtraTrees Feature Importance")
plt.tight_layout()
plt.show()

# ============================================================
# 7) Helper for storing and showing classifier progress
# ============================================================
results = []  # list of dicts: {"model": ..., "train_acc": ..., "test_acc": ...}

def add_result(name, train_acc, test_acc):
    """Store model performance."""
    results.append(
        {"model": name, "train_acc": train_acc, "test_acc": test_acc}
    )

def show_overall_progress():
    """Show a compact table of all models so far."""
    if not results:
        return
    df_res = pd.DataFrame(results)
    best = df_res["test_acc"].max()
    df_res["relative_to_best"] = (df_res["test_acc"] / best).round(3)
    print("\n=== Overall classifier results so far ===")
    print(df_res)
    print("=========================================\n")

# ============================================================
# 8) Gaussian Naive Bayes
# ============================================================
from sklearn.naive_bayes import GaussianNB

print("\n>>> Training GaussianNB...")
clf = GaussianNB().fit(Xtr, Ytr)
y_pred = clf.predict(Xte)
train_acc = clf.score(Xtr, Ytr)
test_acc = clf.score(Xte, Yte)

print("GaussianNB - Train acc:", train_acc, " Test acc:", test_acc)
print(classification_report(Yte, y_pred))

add_result("GaussianNB", train_acc, test_acc)
show_overall_progress()

cm = confusion_matrix(Yte, y_pred, labels=clf.classes_, normalize="true")
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)
disp.plot(values_format="g")
plt.title("GaussianNB - Normalized Confusion Matrix")
plt.show()

# ============================================================
# 9) K-Nearest Neighbors (with step-by-step progress)
# ============================================================
from sklearn.neighbors import KNeighborsClassifier

print("\n>>> Scanning KNN over K = 1..10")
k_values = list(range(1, 11))
knn_c_acc = []

for k in k_values:
    clf = KNeighborsClassifier(n_neighbors=k).fit(Xtr, Ytr)
    train_score = clf.score(Xtr, Ytr)
    test_score = clf.score(Xte, Yte)
    knn_c_acc.append((k, train_score, test_score))
    # Step-by-step progress line:
    print(f"K={k:2d}  Train={train_score:.4f}  Test={test_score:.4f}")

sc = pd.DataFrame(knn_c_acc, columns=["K", "Train Score", "Test Score"])
print("\n=== KNN scores (all K) ===")
print(sc)

# Plot error rate vs K
error_rate = []
for k in k_values:
    clf = KNeighborsClassifier(n_neighbors=k).fit(Xtr, Ytr)
    y_pred = clf.predict(Xte)
    error_rate.append(np.mean(Yte != y_pred))

plt.figure(figsize=(8, 4))
plt.plot(k_values, error_rate, marker="o", markersize=5, color="#FF00AE")
plt.xlabel("K")
plt.ylabel("Error rate")
plt.title("KNN Error Rate vs K")
plt.tight_layout()
plt.show()

# Choose best K based on minimum error
best_k_idx = int(np.argmin(error_rate))
best_k = k_values[best_k_idx]
print(f"\nBest K according to error rate: K={best_k}")

clf = KNeighborsClassifier(n_neighbors=best_k).fit(Xtr, Ytr)
y_pred = clf.predict(Xte)
train_acc = clf.score(Xtr, Ytr)
test_acc = clf.score(Xte, Yte)
print(f"KNN (K={best_k}) - Train acc: {train_acc:.4f}  Test acc: {test_acc:.4f}")
print(classification_report(Yte, y_pred))

add_result(f"KNN(K={best_k})", train_acc, test_acc)
show_overall_progress()

cm = confusion_matrix(Yte, y_pred, labels=clf.classes_, normalize="true")
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)
disp.plot(values_format="g")
plt.title(f"KNN (K={best_k}) - Normalized Confusion Matrix")
plt.show()

# ============================================================
# 10) SVM (RBF)
# ============================================================
from sklearn import svm

print("\n>>> Training SVM (RBF)...")
clf = svm.SVC(kernel="rbf", gamma="scale").fit(Xtr, Ytr)
y_pred = clf.predict(Xte)
train_acc = clf.score(Xtr, Ytr)
test_acc = clf.score(Xte, Yte)
print("SVM (RBF) - Train acc:", train_acc, " Test acc:", test_acc)
print(classification_report(Yte, y_pred))

add_result("SVM (RBF)", train_acc, test_acc)
show_overall_progress()

cm = confusion_matrix(Yte, y_pred, labels=clf.classes_, normalize="true")
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)
disp.plot(values_format="g")
plt.title("SVM (RBF) - Normalized Confusion Matrix")
plt.show()

# ============================================================
# 11) Logistic Regression
# ============================================================
from sklearn.linear_model import LogisticRegression

print("\n>>> Training Logistic Regression...")
clf = LogisticRegression(random_state=42, max_iter=1000).fit(Xtr, Ytr)
y_pred = clf.predict(Xte)
train_acc = clf.score(Xtr, Ytr)
test_acc = clf.score(Xte, Yte)
print("Logistic Regression - Train acc:", train_acc, " Test acc:", test_acc)
print(classification_report(Yte, y_pred))

add_result("Logistic Regression", train_acc, test_acc)
show_overall_progress()

cm = confusion_matrix(Yte, y_pred, labels=clf.classes_, normalize="true")
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)
disp.plot(values_format="g")
plt.title("Logistic Regression - Normalized Confusion Matrix")
plt.show()

# ============================================================
# 12) AdaBoost
# ============================================================
from sklearn.ensemble import AdaBoostClassifier

print("\n>>> Training AdaBoost...")
clf = AdaBoostClassifier(n_estimators=500, random_state=42).fit(Xtr, Ytr)
y_pred = clf.predict(Xte)
train_acc = clf.score(Xtr, Ytr)
test_acc = clf.score(Xte, Yte)
print("AdaBoost - Train acc:", train_acc, " Test acc:", test_acc)
print(classification_report(Yte, y_pred))

add_result("AdaBoost", train_acc, test_acc)
show_overall_progress()

cm = confusion_matrix(Yte, y_pred, labels=clf.classes_, normalize="true")
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)
disp.plot(values_format="g")
plt.title("AdaBoost - Normalized Confusion Matrix")
plt.show()

# ============================================================
# 13) Decision Tree
# ============================================================
from sklearn import tree

print("\n>>> Training Decision Tree...")
clf = tree.DecisionTreeClassifier(random_state=42).fit(Xtr, Ytr)
y_pred = clf.predict(Xte)
train_acc = clf.score(Xtr, Ytr)
test_acc = clf.score(Xte, Yte)
print("Decision Tree - Train acc:", train_acc, " Test acc:", test_acc)
print(classification_report(Yte, y_pred))

add_result("Decision Tree", train_acc, test_acc)
show_overall_progress()

cm = confusion_matrix(Yte, y_pred, labels=clf.classes_, normalize="true")
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)
disp.plot(values_format="g")
plt.title("Decision Tree - Normalized Confusion Matrix")
plt.show()

# ============================================================
# 14) MLPClassifier
# ============================================================
from sklearn.neural_network import MLPClassifier

print("\n>>> Training MLPClassifier...")
clf = MLPClassifier(random_state=1, max_iter=300).fit(Xtr, Ytr)
y_pred = clf.predict(Xte)
train_acc = clf.score(Xtr, Ytr)
test_acc = clf.score(Xte, Yte)
print("MLPClassifier - Train acc:", train_acc, " Test acc:", test_acc)
print(classification_report(Yte, y_pred))

add_result("MLPClassifier", train_acc, test_acc)
show_overall_progress()

cm = confusion_matrix(Yte, y_pred, labels=clf.classes_, normalize="true")
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)
disp.plot(values_format="g")
plt.title("MLPClassifier - Normalized Confusion Matrix")
plt.show()

# ============================================================
# 15) Final summary
# ============================================================
print("\n=== FINAL CLASSIFIER SUMMARY ===")
show_overall_progress()
print("Done.")